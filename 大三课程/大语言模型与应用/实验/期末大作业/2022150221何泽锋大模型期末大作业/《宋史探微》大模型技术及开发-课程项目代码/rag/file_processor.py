import os
import time
import logging
from datetime import datetime
from io import StringIO

import numpy as np
import faiss
import jieba
import docx2txt
from pdfminer.high_level import extract_text_to_fp
from langchain_text_splitters import RecursiveCharacterTextSplitter
from Split_Text import AdvancedTextSplitter

# å‡è®¾è¿™äº›å¯¹è±¡åœ¨åˆ«å¤„åˆå§‹åŒ–å¹¶å¯¼å…¥
# from your_app import app, UPLOAD_FOLDER, allowed_file
from models import EMBED_MODEL
from vector_store import faiss_index, faiss_contents_map, faiss_metadatas_map, faiss_id_order_for_index
from bm25 import BM25_MANAGER



logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')


class FileProcessor:
    def __init__(self):
        self.processed_files = {}

    def clear_files(self):
        self.processed_files = {}

    def add_file(self, file_name, status='ç­‰å¾…å¤„ç†', chunks=0):
        self.processed_files[file_name] = {
            'status': status,
            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'chunks': chunks
        }

    def update_status(self, file_name, status, chunks=None):
        if file_name in self.processed_files:
            self.processed_files[file_name]['status'] = status
            if chunks is not None:
                self.processed_files[file_name]['chunks'] = chunks
    
    def get_file_list(self):
        return [
            f"ğŸ“„ {fname} | {info['status']} ({info['chunks']} ä¸ªæ–‡æœ¬å—)"
            for fname, info in self.processed_files.items()
        ]

file_processor = FileProcessor()


def extract_text(filepath):
    """æ ¹æ®æ–‡ä»¶æ‰©å±•åæå–æ–‡æœ¬å†…å®¹"""
    ext = filepath.lower().rsplit('.', 1)[-1]
    if ext == 'pdf':
        output = StringIO()
        with open(filepath, 'rb') as file:
            extract_text_to_fp(file, output)
        return output.getvalue()
    elif ext in ['txt', 'md']: # å°† markdown æ–‡ä»¶è§†ä¸ºçº¯æ–‡æœ¬æå–
        with open(filepath, 'r', encoding='utf-8') as file:
            return file.read()
    elif ext in ['doc', 'docx']:
        try:
            return docx2txt.process(filepath)
        except Exception as e:
            logging.error(f"å¤„ç†Wordæ–‡æ¡£ {filepath} æ—¶å‡ºé”™: {e}")
            raise ValueError(f"æ— æ³•å¤„ç†Wordæ–‡æ¡£: {e}")
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {filepath}")

# --- æ–°å¢å‡½æ•°ï¼šåœ¨åº”ç”¨å¯åŠ¨æ—¶åŠ è½½æ•´ä¸ªçŸ¥è¯†åº“ ---
def load_knowledge_base(knowledge_base_dir='./pdfs'):
    """
    æ‰«ææŒ‡å®šç›®å½•ä¸‹çš„æ‰€æœ‰ .embedding.npz æ–‡ä»¶ï¼Œå°†å®ƒä»¬åŠ è½½åˆ°å†…å­˜ä¸­ï¼Œ
    å¹¶æ„å»ºåˆå§‹çš„FAISSå’ŒBM25ç´¢å¼•ã€‚
    æ­¤å‡½æ•°åº”åœ¨åº”ç”¨å¯åŠ¨æ—¶è°ƒç”¨ä¸€æ¬¡ã€‚
    """
    global faiss_index, faiss_contents_map, faiss_metadatas_map, faiss_id_order_for_index

    logging.info("--- å¼€å§‹åŠ è½½çŸ¥è¯†åº“ ---")
    all_chunks = []
    all_embeddings_list = []
    all_metadatas = []
    all_original_ids = []

    # æ¸…ç†ä»»ä½•å·²å­˜åœ¨çš„å†…å­˜æ•°æ®ï¼Œç¡®ä¿ä»ä¸€ä¸ªå¹²å‡€çš„çŠ¶æ€å¼€å§‹
    faiss_index = None
    faiss_contents_map.clear()
    faiss_metadatas_map.clear()
    faiss_id_order_for_index.clear()
    BM25_MANAGER.clear()
    file_processor.clear_files()
    
    found_files = 0
    for filename in os.listdir(knowledge_base_dir):
        if filename.endswith(".embedding.npz"):
            embedding_file_path = os.path.join(knowledge_base_dir, filename)
            original_filename = filename.replace(".embedding.npz", "")
            try:
                logging.info(f"æ­£åœ¨åŠ è½½ embedding æ–‡ä»¶: {filename}")
                data = np.load(embedding_file_path, allow_pickle=True)
                
                chunks = data["chunks"].tolist()
                embeddings_np = data["embeddings"]
                metadatas = data["metadatas"].tolist()
                original_ids = data["original_ids"].tolist()
                
                all_chunks.extend(chunks)
                all_embeddings_list.append(embeddings_np)
                all_metadatas.extend(metadatas)
                all_original_ids.extend(original_ids)
                
                file_processor.add_file(original_filename, "å·²åŠ è½½", len(chunks))
                found_files += 1
            except Exception as e:
                logging.error(f"åŠ è½½ embedding æ–‡ä»¶ {filename} å¤±è´¥: {e}")
                file_processor.add_file(original_filename, f"åŠ è½½å¤±è´¥: {e}")

    if not all_chunks:
        logging.warning("æœªæ‰¾åˆ°ä»»ä½• embedding æ–‡ä»¶ã€‚çŸ¥è¯†åº“ä¸ºç©ºã€‚")
        return "çŸ¥è¯†åº“ä¸ºç©ºï¼ŒæœªåŠ è½½ä»»ä½•æ–‡ä»¶ã€‚", []

    logging.info(f"ä» {found_files} ä¸ªæ–‡ä»¶ä¸­åŠ è½½äº†æ•°æ®ï¼Œæ€»è®¡ {len(all_chunks)} ä¸ªæ–‡æœ¬å—ã€‚")

    # 1. æ„å»º FAISS ç´¢å¼•
    logging.info("æ­£åœ¨ä»åŠ è½½çš„æ•°æ®æ„å»ºFAISSç´¢å¼•...")
    all_embeddings_np = np.vstack(all_embeddings_list)
    dimension = all_embeddings_np.shape[1]
    faiss_index = faiss.IndexFlatL2(dimension)
    faiss_index.add(all_embeddings_np)
    
    # 2. å¡«å……å†…å®¹å’Œå…ƒæ•°æ®æ˜ å°„
    for i, original_id in enumerate(all_original_ids):
        faiss_contents_map[original_id] = all_chunks[i]
        faiss_metadatas_map[original_id] = all_metadatas[i]
    
    faiss_id_order_for_index.extend(all_original_ids)
    logging.info(f"FAISSç´¢å¼•æ„å»ºå®Œæˆã€‚æ€»å‘é‡æ•°: {faiss_index.ntotal}")

    # 3. æ„å»º BM25 ç´¢å¼•
    update_bm25_index()
    
    summary = f"æˆåŠŸåŠ è½½ {found_files} ä¸ªæ–‡ä»¶ï¼Œæ€»è®¡ {len(all_chunks)} ä¸ªæ–‡æœ¬å—ã€‚"
    logging.info("--- çŸ¥è¯†åº“åŠ è½½å®Œæˆ ---")
    return summary, file_processor.get_file_list()


# --- é‡æ„å‡½æ•°ï¼šå¢é‡å¼å¤„ç†å¹¶æ·»åŠ æ–°æ–‡ä»¶ ---
def add_files_to_knowledge_base(file_paths, progress=None):
    """
    å¤„ç†ä¸€æ‰¹æ–°æ–‡ä»¶ï¼Œç”Ÿæˆå®ƒä»¬çš„ embeddingï¼Œç„¶åå°†å®ƒä»¬æ·»åŠ åˆ°
    å·²å­˜åœ¨äºå†…å­˜ä¸­çš„FAISSå’ŒBM25ç´¢å¼•ä¸­ã€‚
    """
    if not file_paths:
        return "æœªæä¾›éœ€è¦å¤„ç†çš„æ–‡ä»¶ã€‚", file_processor.get_file_list()

    global faiss_index, faiss_contents_map, faiss_metadatas_map, faiss_id_order_for_index

    processed_results = []
    total_new_chunks = 0
    
    # ä»…ç”¨äºæœ¬æ¬¡ä¸Šä¼ æ‰¹æ¬¡çš„æ•°æ®
    batch_chunks = []
    batch_embeddings_list = []
    batch_metadatas = []
    batch_original_ids = []

    # åœ¨æ–‡ä»¶å¤„ç†å¾ªç¯å¤–éƒ¨å®ä¾‹åŒ–ä¸€æ¬¡åˆ‡åˆ†å™¨
    text_splitter = AdvancedTextSplitter(
        chunk_size=400,
        chunk_overlap=40,
        default_splitting_method="recursive"  # å¯æ”¹ä¸º"semantic"æˆ–"token"
    )

    for idx, file_path in enumerate(file_paths, 1):
        file_name = os.path.basename(file_path)
        file_processor.add_file(file_name) # æ·»åŠ æ–‡ä»¶å¹¶æ ‡è®°ä¸º"ç­‰å¾…å¤„ç†"
        
        try:
            embedding_file = file_path + ".embedding.npz"

            # æ­£å¸¸ä¸Šä¼ æµç¨‹ä¸‹ï¼Œè¿™é‡Œä¸ä¼šå­˜åœ¨ã€‚ä½†ä¸ºäº†å¥å£®æ€§ï¼Œä¿ç•™æ­¤æ£€æŸ¥ã€‚
            if os.path.exists(embedding_file):
                logging.info(f"å‘ç°å·²å­˜åœ¨çš„ embedding æ–‡ä»¶ï¼Œç›´æ¥åŠ è½½: {embedding_file}")
                data = np.load(embedding_file, allow_pickle=True)
                chunks = data["chunks"].tolist()
                embeddings_np = data["embeddings"]
                metadatas = data["metadatas"].tolist()
                original_ids = data["original_ids"].tolist()
            else:
                logging.info(f"æ­£åœ¨å¤„ç†æ–°æ–‡ä»¶: {file_name}")
                if progress: progress(idx / len(file_paths) * 0.5, desc=f"ä» {file_name} æå–æ–‡æœ¬...")
                text = extract_text(file_path)
                
                # æ›¿æ¢åŸæœ‰çš„åˆ‡åˆ†é€»è¾‘
                chunks_data = text_splitter.split_text(text)
                chunks = [chunk['text'] for chunk in chunks_data]
                if not chunks:
                    raise ValueError("æ–‡æ¡£å†…å®¹ä¸ºç©ºæˆ–æ— æ³•æå–æ–‡æœ¬ã€‚")

                doc_id = f"doc_{int(time.time())}_{idx}"
                original_ids = [f"{doc_id}_chunk_{i}" for i in range(len(chunks))]
                metadatas = [{"source": file_name, "doc_id": doc_id} for _ in chunks]
                
                if progress: progress(idx / len(file_paths) * 0.5 + 0.2, desc=f"æ­£åœ¨å‘é‡åŒ– {len(chunks)} ä¸ªæ–‡æœ¬å—...")
                embeddings = EMBED_MODEL.encode(chunks, show_progress_bar=False) # åœ¨æœåŠ¡ç«¯æ—¥å¿—ä¸­ä¿æŒç®€æ´
                embeddings_np = np.array(embeddings).astype('float32')
                
                # ä¸ºæ–°æ–‡ä»¶ä¿å­˜ embedding æ–‡ä»¶ï¼Œä»¥å®ç°æŒä¹…åŒ–
                np.savez_compressed(
                    embedding_file,
                    chunks=np.array(chunks, dtype=object),
                    embeddings=embeddings_np,
                    metadatas=np.array(metadatas, dtype=object),
                    original_ids=np.array(original_ids, dtype=object)
                )
                logging.info(f"å·²ä¿å­˜æ–°çš„ embedding æ–‡ä»¶: {embedding_file}")
            
            # å°†æ­¤æ–‡ä»¶çš„æ•°æ®æ·»åŠ åˆ°æ‰¹å¤„ç†åˆ—è¡¨ä¸­
            batch_chunks.extend(chunks)
            batch_embeddings_list.append(embeddings_np)
            batch_metadatas.extend(metadatas)
            batch_original_ids.extend(original_ids)

            total_new_chunks += len(chunks)
            file_processor.update_status(file_name, "å¤„ç†å®Œæˆ", len(chunks))
            processed_results.append(f"âœ… {file_name}: æˆåŠŸå¤„ç† {len(chunks)} ä¸ªæ–‡æœ¬å—ã€‚")
            
        except Exception as e:
            error_msg = str(e)
            logging.error(f"å¤„ç†æ–‡ä»¶ {file_name} æ—¶å‡ºé”™: {error_msg}")
            file_processor.update_status(file_name, f"å¤„ç†å¤±è´¥: {error_msg}")
            processed_results.append(f"âŒ {file_name}: å¤„ç†å¤±è´¥ - {error_msg}")

    # æ‰€æœ‰æ–‡ä»¶å¤„ç†å®Œæ¯•åï¼Œå¦‚æœ_æœ‰_æ–°æ•°æ®ï¼Œåˆ™æ›´æ–°å…¨å±€ç´¢å¼•
    if batch_chunks:
        logging.info(f"æ­£åœ¨å‘çŸ¥è¯†åº“ä¸­æ·»åŠ  {total_new_chunks} ä¸ªæ–°çš„æ–‡æœ¬å—...")
        if progress: progress(0.8, desc="æ›´æ–°FAISSç´¢å¼•...")
        
        new_embeddings_np = np.vstack(batch_embeddings_list)
        
        # å…³é”®ï¼šå¦‚æœFAISSç´¢å¼•æ˜¯é¦–æ¬¡åˆ›å»ºï¼Œåˆ™åˆå§‹åŒ–å®ƒ
        if faiss_index is None:
            dimension = new_embeddings_np.shape[1]
            faiss_index = faiss.IndexFlatL2(dimension)
            logging.info(f"FAISSç´¢å¼•é¦–æ¬¡åˆ›å»ºï¼Œç»´åº¦ä¸º {dimension}ã€‚")
        else:
            # å…³é”®ï¼šé˜²å¾¡æ€§æ£€æŸ¥ï¼ç¡®ä¿æ–°å‘é‡çš„ç»´åº¦ä¸ç°æœ‰ç´¢å¼•çš„ç»´åº¦åŒ¹é…
            expected_dim = faiss_index.d
            actual_dim = new_embeddings_np.shape[1]
            if expected_dim != actual_dim:
                error_msg = (
                    f"è‡´å‘½é”™è¯¯ï¼šå‘é‡ç»´åº¦ä¸åŒ¹é…ï¼"
                    f"ç°æœ‰FAISSç´¢å¼•éœ€è¦ç»´åº¦ {expected_dim}ï¼Œä½†æ–°æ–‡ä»¶çš„å‘é‡ç»´åº¦ä¸º {actual_dim}ã€‚"
                    f"è¿™å¯èƒ½æ˜¯å› ä¸ºæ›´æ¢äº†æ¨¡å‹æˆ–æ¨¡å‹é…ç½®ä¸ä¸€è‡´ã€‚è¯·è€ƒè™‘æ¸…ç©ºå¹¶é‡å»ºæ•´ä¸ªçŸ¥è¯†åº“ã€‚"
                )
                logging.error(error_msg)
                # æŠ›å‡ºä¸€ä¸ªæ¸…æ™°çš„å¼‚å¸¸ï¼Œè€Œä¸æ˜¯è®©FAISSå´©æºƒ
                raise ValueError(error_msg)

        # ç»´åº¦æ£€æŸ¥é€šè¿‡åï¼Œå®‰å…¨åœ°æ·»åŠ æ–°å‘é‡
        faiss_index.add(new_embeddings_np)
        
        # æ›´æ–°å†…å®¹å’Œå…ƒæ•°æ®æ˜ å°„
        for i, original_id in enumerate(batch_original_ids):
            faiss_contents_map[original_id] = batch_chunks[i]
            faiss_metadatas_map[original_id] = batch_metadatas[i]
        
        faiss_id_order_for_index.extend(batch_original_ids)
        logging.info(f"FAISSç´¢å¼•æ›´æ–°å®Œæˆã€‚æ€»å‘é‡æ•°: {faiss_index.ntotal}")

        # ä½¿ç”¨æ‰€æœ‰æ–‡æ¡£ï¼ˆæ—§+æ–°ï¼‰é‡å»ºBM25ç´¢å¼•
        if progress: progress(0.95, desc="æ›´æ–°BM25ç´¢å¼•...")
        update_bm25_index()

    summary = f"\nå¤„ç†äº† {len(file_paths)} ä¸ªæ–‡ä»¶ï¼Œæ–°å¢ {total_new_chunks} ä¸ªæ–‡æœ¬å—ã€‚"
    processed_results.append(summary)
    
    return "\n".join(processed_results), file_processor.get_file_list()


def update_bm25_index():
    """ä»å†…å­˜ä¸­å®Œæ•´çš„æ–‡æ¡£é›†åˆé‡å»ºBM25ç´¢å¼•"""
    global faiss_contents_map, faiss_id_order_for_index
    try:
        doc_ids = faiss_id_order_for_index
        if not doc_ids:
            logging.warning("æ²¡æœ‰å¯ç”¨äºæ„å»ºBM25ç´¢å¼•çš„æ–‡æ¡£ã€‚")
            BM25_MANAGER.clear()
            return False
        
        documents = [faiss_contents_map.get(doc_id, "") for doc_id in doc_ids]
        
        valid_docs_with_ids = [(doc_id, doc) for doc_id, doc in zip(doc_ids, documents) if doc]
        if not valid_docs_with_ids:
            logging.warning("æ²¡æœ‰æœ‰æ•ˆçš„æ–‡æ¡£å†…å®¹å¯ç”¨äºBM25ç´¢å¼•ã€‚")
            BM25_MANAGER.clear()
            return False
            
        final_doc_ids, final_documents = zip(*valid_docs_with_ids)

        BM25_MANAGER.build_index(list(final_documents), list(final_doc_ids))
        logging.info(f"BM25ç´¢å¼•é‡å»ºæˆåŠŸã€‚å…±ç´¢å¼• {len(final_doc_ids)} ä¸ªæ–‡æ¡£ã€‚")
        return True
    except Exception as e:
        logging.error(f"æ›´æ–°BM25ç´¢å¼•å¤±è´¥: {e}")
        return False
